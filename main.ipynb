{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data\n",
    "The following code block allows a user to get data files and utility scripts beginning with only main.ipynb either locally or on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget import exists\n",
      "Examples:\n",
      "    Total: 8641\n",
      "    P: 367 (4.25% of total)\n",
      "    S: 3965 (45.89% of total)\n",
      "    W: 4309 (49.87% of total)\n",
      "\n",
      "Examples:\n",
      "    Total: 8641\n",
      "    P: 614 (7.11% of total)\n",
      "    S: 3294 (38.12% of total)\n",
      "    W: 4733 (54.77% of total)\n",
      "\n",
      "Examples:\n",
      "    Total: 17282\n",
      "    P: 981 (5.68% of total)\n",
      "    S: 7259 (42.00% of total)\n",
      "    W: 9042 (52.32% of total)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "from scripts.utils import *\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    __import__(\"wget\")\n",
    "    import wget\n",
    "    print(\"wget import exists\")\n",
    "    if(not path.exists(\"data\")):\n",
    "        print(\"Making Directory data\")\n",
    "        os.mkdir(\"data\")   \n",
    "    if(not path.exists(\"data/control.xls\")):\n",
    "        print(\"Downloading control.xls\")\n",
    "        wget.download(\"https://raw.githubusercontent.com/smithandrewk/sleep/main/data/control.xls\",\"data/control.xls\")\n",
    "    if(not path.exists(\"data/deprivation.xls\")):\n",
    "        print(\"Downloading deprivation.xls\")\n",
    "        wget.download(\"https://raw.githubusercontent.com/smithandrewk/sleep/main/data/deprivation.xls\",\"data/deprivation.xls\")\n",
    "    if(not path.exists(\"scripts\")):\n",
    "        print(\"Making Directory scripts\")\n",
    "        os.mkdir(\"scripts\")   \n",
    "    if(not path.exists(\"scripts/utils.py\")):\n",
    "        print(\"Downloading utils.py\")\n",
    "        wget.download(\"https://raw.githubusercontent.com/smithandrewk/sleep/main/scripts/utils.py\",\"scripts/utils.py\")\n",
    "except ImportError:\n",
    "    print(\"wget import does not exist, using python2.7 wget\")\n",
    "    if(not path.exists(\"data\")):\n",
    "        print(\"Making Directory data\")\n",
    "        !mkdir data   \n",
    "    if(not path.exists(\"data/control.xls\")):\n",
    "        print(\"control.xls not in data\")\n",
    "        !wget -O data/control.xls https://raw.githubusercontent.com/smithandrewk/sleep/main/data/control.xls\n",
    "    if(not path.exists(\"data/deprivation.xls\")):\n",
    "        print(\"deprivation.xls not in data\")\n",
    "        !wget -O data/deprivation.xls https://raw.githubusercontent.com/smithandrewk/sleep/main/data/deprivation.xls\n",
    "    if(not path.exists(\"scripts\")):\n",
    "        print(\"Making Directory scripts\")\n",
    "        !mkdir scripts\n",
    "    if(not path.exists(\"scripts/utils.py\")):\n",
    "        !wget -O scripts/utils.py https://raw.githubusercontent.com/smithandrewk/sleep/main/scripts/utils.py\n",
    "control = preprocess(\"control\")\n",
    "deprivation = preprocess(\"deprivation\")\n",
    "control_deprivation = pd.concat([control,deprivation])\n",
    "control_deprivation.to_csv(\"data/controldeprivation_preprocessed.csv\",index=False)\n",
    "p,s,w = class_count(control)\n",
    "p,s,w = class_count(deprivation)\n",
    "p,s,w = class_count(control_deprivation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_filename = 'controldeprivation' # or 'control' or 'deprivation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "    Total: 17282\n",
      "    P: 981 (5.68% of total)\n",
      "    S: 7259 (42.00% of total)\n",
      "    W: 9042 (52.32% of total)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/\"+target_filename+\"_preprocessed.csv\")\n",
    "\n",
    "## Balancing\n",
    "ps = df.loc[df[\"Class\"]==0]\n",
    "ss = df.loc[df[\"Class\"]==1]\n",
    "ws = df.loc[df[\"Class\"]==2]\n",
    "for i in range(int(w/p)):\n",
    "  df = pd.concat([df,ps])\n",
    "p,s,w = class_count(df)\n",
    "\n",
    "df.to_csv(\"data/\"+target_filename+\"_balanced.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/\"+target_filename+\"_balanced.csv\")\n",
    "\n",
    "# Use a utility from sklearn to split and shuffle our dataset.\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('Class'))\n",
    "val_labels = np.array(val_df.pop('Class'))\n",
    "test_labels = np.array(test_df.pop('Class'))\n",
    "\n",
    "train_features = np.array(train_df)\n",
    "val_features = np.array(val_df)\n",
    "test_features = np.array(test_df)\n",
    "\n",
    "total = p + s + w\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_for_p = (1 / p)*(total)/2.0 \n",
    "weight_for_w = (1 / w)*(total)/2.0\n",
    "weight_for_s = (1 / s)*(total)/2.0\n",
    "\n",
    "\n",
    "class_weight = {0: weight_for_p, 1: weight_for_s, 2: weight_for_w}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_p))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_s))\n",
    "print('Weight for class 2: {:.2f}'.format(weight_for_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(INPUT_FEATURES,hln=20,EPOCHS=100,BATCH_SIZE=200,weights=False):\n",
    "    model = get_compiled_model(hln,INPUT_FEATURES=INPUT_FEATURES,dropout=True)\n",
    "    \"\"\"\n",
    "    We one-hot encode the targets. Mathematically, this is good for calculating\n",
    "    loss. CategoricalCrossEntropy simplifies to a negative log when targets are\n",
    "    one-hot encoded. However, I simply recieved an error from model.fit when I \n",
    "    did not one-hot encode.\n",
    "      @y : targets\n",
    "      @depth : number of targets\n",
    "    \"\"\"\n",
    "    # Callback choices\n",
    "    plot_losses = TrainingPlot()\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='categorical_accuracy', \n",
    "        verbose=1,\n",
    "        patience=100,\n",
    "        mode='max',\n",
    "        restore_best_weights=True)\n",
    "    tensorboard = TensorBoard(log_dir='logs/{}'.format(time()))\n",
    "    if(weights):\n",
    "        baseline_history = model.fit(\n",
    "            train_features,\n",
    "            tf.one_hot(train_labels,depth=3),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            validation_data=(val_features, tf.one_hot(val_labels,depth=3)),\n",
    "            callbacks=[early_stopping],\n",
    "            class_weight=class_weight)\n",
    "    else:\n",
    "        baseline_history = model.fit(\n",
    "        train_features,\n",
    "        tf.one_hot(train_labels,depth=3),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(val_features, tf.one_hot(val_labels,depth=3)),\n",
    "        callbacks=[early_stopping])\n",
    "    train_predictions_baseline = model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "    test_predictions_baseline = model.predict(test_features, batch_size=BATCH_SIZE)\n",
    "    baseline_results = model.evaluate(test_features, tf.one_hot(test_labels,depth=3),\n",
    "                                      batch_size=BATCH_SIZE, verbose=0)\n",
    "    plot_cm(tf.one_hot(test_labels,depth=3).numpy().argmax(axis=1),test_predictions_baseline.argmax(axis=1),baseline_results,hln)\n",
    "    date = strftime('%X %x').replace(\"/\",\"\").split()\n",
    "    plt.savefig(\"figures/\"+str(date[1])+\"@\"+str(date[0][:5].replace(\":\",\"\"))+\"_\"+str(hln)+\".png\",bbox_inches='tight')\n",
    "    return baseline_history,baseline_results,date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "trains = []\n",
    "tests = []\n",
    "for i in range(5):\n",
    "    train_history,test_history,date = train(INPUT_FEATURES=(train_features.shape[-1],),\n",
    "                                   hln=200,\n",
    "                                   EPOCHS=1000,\n",
    "                                   weights=True)\n",
    "    trains.append(train_history)\n",
    "    tests.append(test_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = [test[1] for test in tests]\n",
    "print(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(train_history)\n",
    "plt.savefig(\"figures/\"+str(date[1])+\"@\"+str(date[0][:5].replace(\":\",\"\"))+\"_training.png\",bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
