{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Get Data\n",
    "The following code block allows a user to get data files and utility scripts beginning with only main.ipynb either locally or on Google Colab."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "try:\n",
    "    __import__(\"wget\")\n",
    "    import wget\n",
    "    print(\"wget import exists\")\n",
    "    if(not path.exists(\"data\")):\n",
    "        print(\"Making Directory data\")\n",
    "        os.mkdir(\"data\")   \n",
    "    if(not path.exists(\"data/control.xls\")):\n",
    "        print(\"Downloading control.xls\")\n",
    "        wget.download(\"https://raw.githubusercontent.com/smithandrewk/sleep/main/data/control.xls\",\"data/control.xls\")\n",
    "    if(not path.exists(\"data/deprivation.xls\")):\n",
    "        print(\"Downloading deprivation.xls\")\n",
    "        wget.download(\"https://raw.githubusercontent.com/smithandrewk/sleep/main/data/deprivation.xls\",\"data/deprivation.xls\")\n",
    "    if(not path.exists(\"scripts\")):\n",
    "        print(\"Making Directory scripts\")\n",
    "        os.mkdir(\"scripts\")   \n",
    "    if(not path.exists(\"scripts/utils.py\")):\n",
    "        print(\"Downloading utils.py\")\n",
    "        wget.download(\"https://raw.githubusercontent.com/smithandrewk/sleep/main/scripts/utils.py\",\"scripts/utils.py\")\n",
    "except ImportError:\n",
    "    print(\"wget import does not exist, using python2.7 wget\")\n",
    "    if(not path.exists(\"data\")):\n",
    "        print(\"Making Directory data\")\n",
    "        !mkdir data   \n",
    "    if(not path.exists(\"data/control.xls\")):\n",
    "        print(\"control.xls not in data\")\n",
    "        !wget -O data/control.xls https://raw.githubusercontent.com/smithandrewk/sleep/main/data/control.xls\n",
    "    if(not path.exists(\"data/deprivation.xls\")):\n",
    "        print(\"deprivation.xls not in data\")\n",
    "        !wget -O data/deprivation.xls https://raw.githubusercontent.com/smithandrewk/sleep/main/data/deprivation.xls\n",
    "    if(not path.exists(\"scripts\")):\n",
    "        print(\"Making Directory scripts\")\n",
    "        !mkdir scripts\n",
    "    if(not path.exists(\"scripts/utils.py\")):\n",
    "        !wget -O scripts/utils.py https://raw.githubusercontent.com/smithandrewk/sleep/main/scripts/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import *\n",
    "df = preprocess(\"control\")\n",
    "## Statistics\n",
    "# df.describe() # nice for obtaining statistics over dataframe\n",
    "p,s,w = np.bincount(df['Class'])\n",
    "total = p + s + w\n",
    "print('Examples:\\n    Total: {}\\n    P: {} ({:.2f}% of total)\\n    S: {} ({:.2f}% of total)\\n    W: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, p, 100 * p / total,s,100 * s / total,w,100 * w / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv(\"data/control_preprocessed.csv\")\n",
    "\n",
    "# Use a utility from sklearn to split and shuffle our dataset.\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "# train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('Class'))\n",
    "p_train_labels = train_labels == 0\n",
    "s_train_labels = train_labels == 1\n",
    "w_train_labels = train_labels == 2\n",
    "\n",
    "# val_labels = np.array(val_df.pop('Class'))\n",
    "test_labels = np.array(test_df.pop('Class'))\n",
    "\n",
    "train_features = np.array(train_df)\n",
    "# val_features = np.array(val_df)\n",
    "test_features = np.array(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "def get_compiled_model(n,dropout=True):\n",
    "    \"\"\"\n",
    "    Function to create model. This is a sequential model, meaning layers execute\n",
    "    one after the other. We have an input shape corresponding to the feature set,\n",
    "    one hidden layer with 10 neurons and a relu activation, then an output layer\n",
    "    with 3 neurons and a sigmoid activation function. We compute loss with\n",
    "    categorical crossentropy and optimize with adam, which I believe is something\n",
    "    about an adaptive learning rate. I do not know what the parameter from_logits\n",
    "    is about.\n",
    "    \"\"\"\n",
    "    if(dropout):\n",
    "        model = tf.keras.Sequential([\n",
    "        keras.layers.Dense(n, activation='relu',input_shape=(train_features.shape[-1],)),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(3, activation='sigmoid')\n",
    "        ])\n",
    "    else:\n",
    "        model = tf.keras.Sequential([\n",
    "        keras.layers.Dense(n, activation='relu',input_shape=(train_features.shape[-1],)),\n",
    "        tf.keras.layers.Dense(3, activation='sigmoid')\n",
    "        ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 200\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='accuracy', \n",
    "    verbose=1,\n",
    "    patience=100,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_compiled_model(20,dropout=False)\n",
    "\"\"\"\n",
    "We one-hot encode the targets. Mathematically, this is good for calculating\n",
    "loss. CategoricalCrossEntropy simplifies to a negative log when targets are\n",
    "one-hot encoded. However, I simply recieved an error from model.fit when I \n",
    "did not one-hot encode.\n",
    "  @y : targets\n",
    "  @depth : number of targets\n",
    "\"\"\"\n",
    "baseline_history = model.fit(\n",
    "    train_features,\n",
    "    tf.one_hot(train_labels,depth=3),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_baseline = model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "test_predictions_baseline = model.predict(test_features, batch_size=BATCH_SIZE)\n",
    "\n",
    "baseline_results = model.evaluate(test_features, tf.one_hot(test_labels,depth=3),\n",
    "                                  batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(model.metrics_names, baseline_results):\n",
    "  print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(tf.one_hot(test_labels,depth=3).numpy().argmax(axis=1), test_predictions_baseline.argmax(axis=1))"
   ]
  }
 ]
}